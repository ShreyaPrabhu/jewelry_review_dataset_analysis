{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c14ab835",
   "metadata": {},
   "source": [
    "## Python Version 3.7.0\n",
    "\n",
    "### Packages version \n",
    "1. PyTorch - 1.12.1\n",
    "2. Gensim - 4.2.0\n",
    "3. Pandas - 1.1.5\n",
    "4. Numpy - 1.21.6\n",
    "5. Emoji - 2.0.0\n",
    "6. Textblob - 0.15.3\n",
    "7. NLTK - 3.7\n",
    "8. Sklearn - 1.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e062fe59",
   "metadata": {},
   "source": [
    "## Packages required for executing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde030fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "import emoji\n",
    "import contractions\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490dbedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54566c",
   "metadata": {},
   "source": [
    "# 1. Dataset Generation\n",
    "\n",
    "This step includes Downloading dataset and Dataset cleaning. Dataset cleaning techniques used are similar to the ones used in HW1. This is to ensure effective comparison of model performances.<br>\n",
    "Cleaning techniques used -\n",
    "1. Drop nan values \n",
    "2. Convert column data types to string(reviews) and integer(rating)\n",
    "3. Select 100k samples such that 20k of the samples belong to each rating class\n",
    "4. Remove html tags \n",
    "5. Remove external links \n",
    "6. Remove extra white spaces\n",
    "7. Remove accents \n",
    "8. Replace emojis with text\n",
    "9. Replace contractions and Spell correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6efad3",
   "metadata": {},
   "source": [
    "##### Read data from data.tsv and drop columns that are not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7cd404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath =  \"data.tsv\"\n",
    "\n",
    "amazon_reviews = pd.read_csv(filepath, sep = \"\\t\", \n",
    "                             skip_blank_lines = True, \n",
    "                             error_bad_lines=False, \n",
    "                             warn_bad_lines = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e06d7375",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews.drop(\n",
    "    columns=['marketplace', \n",
    "             'customer_id', \n",
    "             'review_id', \n",
    "             'product_id', \n",
    "             'product_title', \n",
    "             'product_category', \n",
    "             'helpful_votes', \n",
    "             'total_votes', \n",
    "             'vine', \n",
    "             'verified_purchase', \n",
    "             'review_headline', \n",
    "             'review_date', \n",
    "             'product_parent'], inplace = True)\n",
    "\n",
    "\n",
    "amazon_reviews.rename(columns=\n",
    "                      {'review_body': 'reviews', \n",
    "                       'star_rating': 'star_rating'}, \n",
    "                      inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ea8d7",
   "metadata": {},
   "source": [
    "##### Drop nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da699a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews = amazon_reviews[amazon_reviews['star_rating'].notna()]\n",
    "\n",
    "amazon_reviews = amazon_reviews[amazon_reviews['reviews'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e6d21",
   "metadata": {},
   "source": [
    "##### Convert column data types to string(reviews) and integer(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9503d607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IntegerArray>\n",
       "[5, 1, 4, 3, 2]\n",
       "Length: 5, dtype: Int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_reviews['star_rating'] = pd.to_numeric(\n",
    "    amazon_reviews['star_rating']).astype('Int64')\n",
    "\n",
    "amazon_reviews.star_rating.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d1d06",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711cdf5f",
   "metadata": {},
   "source": [
    "##### Select 100k samples such that 20k of the samples belong to each rating class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71957aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "star_rating\n",
       "1    20000\n",
       "2    20000\n",
       "3    20000\n",
       "4    20000\n",
       "5    20000\n",
       "Name: star_rating, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_count = 20000\n",
    "\n",
    "sampled_reviews = amazon_reviews.groupby('star_rating').apply(\n",
    "    lambda x: x.sample(n=sample_count, \n",
    "                       random_state=42)).reset_index(drop = True)\n",
    "\n",
    "sampled_reviews.groupby(['star_rating'])['star_rating'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fd86a0",
   "metadata": {},
   "source": [
    "##### Remove html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6914fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews['reviews'] = sampled_reviews['reviews'].str.replace(\n",
    "    r'<[^<>]*>', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d859cc4",
   "metadata": {},
   "source": [
    "##### Remove external links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7767c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews['reviews'] = sampled_reviews['reviews'].apply(\n",
    "    lambda x: re.sub(r'http\\S+', ' ', str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16317b8b",
   "metadata": {},
   "source": [
    "##### Remove extra white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27ea58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews['reviews'] = sampled_reviews['reviews'].str.strip()\n",
    "\n",
    "sampled_reviews['reviews'] = sampled_reviews['reviews'].replace(\n",
    "    r'\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd30e6bb",
   "metadata": {},
   "source": [
    "##### Remove accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eb64b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_accent = lambda text: normalize(\"NFKD\", text).encode(\n",
    "    \"ascii\", \"ignore\").decode(\n",
    "    \"utf-8\", \"ignore\")\n",
    "\n",
    "sampled_reviews[\"reviews\"] = sampled_reviews[\"reviews\"].apply(\n",
    "    remove_accent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5634e9f",
   "metadata": {},
   "source": [
    "##### Replace emojis with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b3f6ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews['reviews'] = sampled_reviews['reviews'].apply(\n",
    "    lambda x: emoji.demojize(x))\n",
    "\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r';\\(', \n",
    "                                   value='sad, angry', \n",
    "                                   regex=True, inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r';-\\(', \n",
    "                                   value='sad, angry', \n",
    "                                   regex=True, inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r':\\(', \n",
    "                                   value='sad, angry', \n",
    "                                   regex=True, inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r':-\\(', \n",
    "                                   value='sad, angry', \n",
    "                                   regex=True, inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r':\\'\\(', \n",
    "                                   value='sad, angry', \n",
    "                                   regex=True, inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r';\\'\\(', \n",
    "                                   value='sad, angry', \n",
    "                                   regex=True, inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r';*\\(', \n",
    "                                   value='sad, angry', \n",
    "                                   regex=True, inplace = True)\n",
    "\n",
    "\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r':\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r':\\'\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r';\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r':-\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r';-\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r';-\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r':o\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r';o\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r';0\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r';O\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r':=\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r';=\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r':^\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r':d\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r':0\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r':~\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)\n",
    "sampled_reviews[\"reviews\"].replace(to_replace=r';*\\)', \n",
    "                                   value='Happy', regex=True, \n",
    "                                   inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e52e87",
   "metadata": {},
   "source": [
    "##### Replace contractions and Spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f994d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_reviews['reviews'] = sampled_reviews['reviews'].apply(\n",
    "    lambda x: contractions.fix(x))\n",
    "\n",
    "def remove_contractions(phrase):\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "sampled_reviews['reviews'] = sampled_reviews['reviews'].apply(\n",
    "    remove_contractions)\n",
    "\n",
    "\n",
    "sampled_reviews['reviews'] = sampled_reviews['reviews'].str.replace(\n",
    "    '[^a-zA-Z ]', ' ')\n",
    "sampled_reviews['reviews'] = sampled_reviews['reviews'].str.strip()\n",
    "sampled_reviews['reviews'] = sampled_reviews['reviews'].replace(\n",
    "    r'\\s+', ' ', regex=True)\n",
    "\n",
    "rx = re.compile(r'([^\\W\\d_])\\1{2,}')\n",
    "\n",
    "def word_correction(text):\n",
    "    correct = re.sub(r'[^\\W\\d_]+', \n",
    "                     lambda x: Word(\n",
    "                         rx.sub(r'\\1\\1', x.group())).correct() \n",
    "                     if rx.search(\n",
    "                         x.group()) else x.group(), text)\n",
    "    return correct\n",
    "\n",
    "sampled_reviews['reviews'] = sampled_reviews['reviews'].apply(\n",
    "    word_correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8573e268",
   "metadata": {},
   "source": [
    "# 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e03265b",
   "metadata": {},
   "source": [
    "### a) “word2vec-google-news-300” Word2Vec model\n",
    "\n",
    "Following are tried to check semantic similarities\n",
    "1. weak + break ~ flimsy\n",
    "2. terrific ~ superb\n",
    "3. woman + royal ~ princess\n",
    "4. king - man + woman ~ queen \n",
    "5. execellent ~ outstanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aff6aab",
   "metadata": {},
   "source": [
    "##### weak + break ~ flimsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46233607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31923553\n",
      "[('weak', 0.8075981736183167), ('break', 0.7346563339233398), ('weaker', 0.6097529530525208), ('Weak', 0.5651276707649231), ('weakening', 0.5610632300376892)]\n"
     ]
    }
   ],
   "source": [
    "vec_weak = wv['weak']\n",
    "vec_break = wv['break']\n",
    "vec_flimsy = wv['flimsy']\n",
    "\n",
    "result = vec_weak + vec_break\n",
    "\n",
    "print(np.dot(result,vec_flimsy)/(norm(result)*norm(vec_flimsy)))\n",
    "print(wv.most_similar(positive=[result], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a48d2",
   "metadata": {},
   "source": [
    "##### terrific ~ superb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a44e7d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7476986\n"
     ]
    }
   ],
   "source": [
    "vec_terrific = wv[\"terrific\"]\n",
    "vec_superb = wv[\"superb\"]\n",
    "\n",
    "print(np.dot(vec_terrific,vec_superb)/(norm(vec_terrific)*norm(vec_superb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acddf78a",
   "metadata": {},
   "source": [
    "##### woman + royal ~ princess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e84becc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63185287\n"
     ]
    }
   ],
   "source": [
    "vec_woman = wv[\"woman\"]\n",
    "vec_royal = wv[\"royal\"]\n",
    "vec_princess = wv[\"princess\"]\n",
    "\n",
    "result = vec_woman + vec_royal\n",
    "\n",
    "print(np.dot(result,vec_princess)/(norm(result)*norm(vec_princess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cedbfa4",
   "metadata": {},
   "source": [
    "##### king - man + woman ~ queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01868f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73005176\n"
     ]
    }
   ],
   "source": [
    "vec_king = wv[\"king\"]\n",
    "vec_man = wv[\"man\"]\n",
    "vec_queen = wv[\"queen\"]\n",
    "vec_woman = wv[\"woman\"]\n",
    "\n",
    "result = vec_king - vec_man + vec_woman\n",
    "\n",
    "print(np.dot(result,vec_queen)/(norm(result)*norm(vec_queen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25c6ea",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2266de8f",
   "metadata": {},
   "source": [
    "##### execellent ~ outstanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d291e36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5567486\n"
     ]
    }
   ],
   "source": [
    "vec_excellent = wv[\"excellent\"]\n",
    "vec_outstanding = wv[\"outstanding\"]\n",
    "\n",
    "print(np.dot(vec_excellent,vec_outstanding)/(norm(vec_excellent)*norm(vec_outstanding)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96801c78",
   "metadata": {},
   "source": [
    "### b) Train a Word2Vec model using your own dataset. Set the embedding size to be 300 and the window size to be 11. You can also consider a minimum word count of 10. Check the semantic similarities for the same two examples in part (a). What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better? For the rest of this assignment, use the pretrained “word2vec-googlenews-300” Word2Ve features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a64884ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JewellryReviewCorpus:\n",
    "    def __iter__(self):\n",
    "        for index, row in sampled_reviews.iterrows():\n",
    "            yield utils.simple_preprocess(row['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60e2bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = JewellryReviewCorpus()\n",
    "jewellry_review_model = gensim.models.Word2Vec(\n",
    "    sentences=sentences, window=11, vector_size=300, min_count = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254215c1",
   "metadata": {},
   "source": [
    "##### weak + break ~ flimsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62058d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64387095\n",
      "[('break', 0.9472134709358215), ('bend', 0.7155886292457581), ('weak', 0.6939854621887207), ('snap', 0.6832017302513123), ('breaks', 0.6801912188529968)]\n"
     ]
    }
   ],
   "source": [
    "vec_weak = jewellry_review_model.wv['weak']\n",
    "vec_break = jewellry_review_model.wv['break']\n",
    "vec_flimsy = jewellry_review_model.wv['flimsy']\n",
    "result = vec_weak + vec_break\n",
    "print(np.dot(result,vec_flimsy)/(norm(result)*norm(vec_flimsy)))\n",
    "print(jewellry_review_model.wv.most_similar(positive=[result], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b14485",
   "metadata": {},
   "source": [
    "##### terrific ~ superb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a141ac45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41385764\n"
     ]
    }
   ],
   "source": [
    "vec_terrific = jewellry_review_model.wv[\"terrific\"]\n",
    "vec_superb = jewellry_review_model.wv[\"superb\"]\n",
    "\n",
    "print(np.dot(vec_terrific,vec_superb)/(norm(\n",
    "    vec_terrific)*norm(vec_superb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582a7a3c",
   "metadata": {},
   "source": [
    "##### woman + royal ~ princess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d78ce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31565464\n"
     ]
    }
   ],
   "source": [
    "vec_woman = jewellry_review_model.wv[\"woman\"]\n",
    "vec_royal = jewellry_review_model.wv[\"royal\"]\n",
    "vec_princess = jewellry_review_model.wv[\"princess\"]\n",
    "\n",
    "result = vec_woman + vec_royal\n",
    "\n",
    "print(np.dot(result,vec_princess)/(norm(result)*norm(vec_princess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6e8dfb",
   "metadata": {},
   "source": [
    "##### king - man + woman ~ queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cce6f9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.276741\n"
     ]
    }
   ],
   "source": [
    "vec_king = jewellry_review_model.wv[\"king\"]\n",
    "vec_man = jewellry_review_model.wv[\"man\"]\n",
    "vec_queen = jewellry_review_model.wv[\"queen\"]\n",
    "vec_woman = jewellry_review_model.wv[\"woman\"]\n",
    "\n",
    "result = vec_king - vec_man + vec_woman\n",
    "\n",
    "print(np.dot(result,vec_queen)/(norm(result)*norm(vec_queen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7188ea",
   "metadata": {},
   "source": [
    "##### execellent ~ outstanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31cf1fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8027048\n"
     ]
    }
   ],
   "source": [
    "vec_excellent = jewellry_review_model.wv[\"excellent\"]\n",
    "vec_outstanding = jewellry_review_model.wv[\"outstanding\"]\n",
    "\n",
    "print(np.dot(vec_excellent,vec_outstanding)/(\n",
    "    norm(vec_excellent)*norm(vec_outstanding)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9cb90",
   "metadata": {},
   "source": [
    "### Summary and Observations - \n",
    "\n",
    "Following is summary of various semantic similarities that were tested with Google Word2Vec and Amazon Reviews Dataset Word2Vec. \n",
    "\n",
    "<br>\n",
    "Various words were tried to check if the models were able to capture semantic similarities. Cosine similarity was used\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Semantics                  | Google Word2Vec   |  Amazon Review Dataset Word2Vec\n",
    "| -------------------------- | ----------------- | ----------------------------------\n",
    "| weak + break ~ flimsy      | 0.31923553        |  0.64406204\n",
    "| terrific ~ superb          | 0.7476986         |  0.41486466\n",
    "| woman + royal ~ princess   | 0.63185287        |  0.31566978\n",
    "| king - man + woman ~ queen | 0.73005176        |  0.27788362\n",
    "| execellent ~ outstanding   | 0.73005176        |  0.8023542\n",
    "\n",
    "\n",
    "##### weak + break ~ flimsy\n",
    "These 3 words occur a lot in Amazon review dataset and mostly co-occur in several reviews. Because of this, we can see that Amazon Review Dataset Word2Vec returned vectors that have high similarity score(0.64) for weak + break and flimsy. But when the same is sent through Google Word2Vec, it has lower score of 0.31. \n",
    "\n",
    "When we call most_similar function for Google Word2Vec for weak + break, the words returned were weak, Weak, breaks etc. But when same was tried with Amazon Review Dataset Word2Vec, it returned break, bend, snap etc. The word and its context play major role in identifying word semantic similarities.\n",
    "\n",
    "##### terrific ~ superb, woman + royal ~ princess, king - man + woman ~ queen\n",
    "Amazon Review Dataset Word2Vec is trained on limited vocabulary related to reviews regarding Jewellery products. Hence its ability to perform well on generic words is less. For the above examples which include generic words(words that are not specific to jewellery in general), Google Word2Vec was able to capture the word similarity for the words and returned vectors that had high cosine similarity whereas Amazon Review Dataset Word2Vec was not able to capture the word similarity or semantic similarity and gave low scores\n",
    "\n",
    "##### execellent ~ outstanding\n",
    "This set of words are present in both the vocabularies and it can be seen that both Google Word2Vec and Amazon Reviews Dataset Word2Vec have been able to capture the semantic similarities of the words. A cosine similarity of 0.73 and 0.80 was obtained showing high similarities in the words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0315b7a2",
   "metadata": {},
   "source": [
    "# Train and Test Split\n",
    "\n",
    "1. We first tokenize the reviews and store it in a new dataframe column tokenized_reviews. \n",
    "2. gensim.utils.simple_preprocess function is used to data clean and tokenize reviews. This also converts words to lower case\n",
    "3. The data is split in 80% train and 20% test \n",
    "\n",
    "Train and Test split is done at the beginning so that all the models can be tested against same train samples and test samples. This will also help us understand and compare model accuracies easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63735659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>reviews</th>\n",
       "      <th>tokenized_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>These are horrible Very cloudy no shine at all...</td>\n",
       "      <td>[these, are, horrible, very, cloudy, no, shine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I returned it for it was too thin and the clos...</td>\n",
       "      <td>[returned, it, for, it, was, too, thin, and, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Nothing like the picture studs were as big as ...</td>\n",
       "      <td>[nothing, like, the, picture, studs, were, as,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I ordered this item paying extra for Guarantee...</td>\n",
       "      <td>[ordered, this, item, paying, extra, for, guar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I had seen something very similar to this item...</td>\n",
       "      <td>[had, seen, something, very, similar, to, this...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   star_rating                                            reviews  \\\n",
       "0            1  These are horrible Very cloudy no shine at all...   \n",
       "1            1  I returned it for it was too thin and the clos...   \n",
       "2            1  Nothing like the picture studs were as big as ...   \n",
       "3            1  I ordered this item paying extra for Guarantee...   \n",
       "4            1  I had seen something very similar to this item...   \n",
       "\n",
       "                                   tokenized_reviews  \n",
       "0  [these, are, horrible, very, cloudy, no, shine...  \n",
       "1  [returned, it, for, it, was, too, thin, and, t...  \n",
       "2  [nothing, like, the, picture, studs, were, as,...  \n",
       "3  [ordered, this, item, paying, extra, for, guar...  \n",
       "4  [had, seen, something, very, similar, to, this...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_reviews['reviews'] = sampled_reviews['reviews'].astype(str)\n",
    "sampled_reviews['tokenized_reviews'] = sampled_reviews['reviews'].apply(\n",
    "    lambda x: gensim.utils.simple_preprocess(x))\n",
    "sampled_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcf59c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count = 16000\n",
    "train_data = sampled_reviews.groupby('star_rating').apply(\n",
    "    lambda x: x.sample(n=train_count, random_state=42))\n",
    "train_index_tuple_list = train_data.index.values.tolist()\n",
    "train_index_list = [x[1] for x in train_index_tuple_list]\n",
    "test_data = sampled_reviews[~sampled_reviews.index.isin(train_index_list)]\n",
    "train_data.reset_index(drop=True, inplace = True)\n",
    "test_data.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "740f8029",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_google = set(wv.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129ce5c",
   "metadata": {},
   "source": [
    "# 3. Simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349103de",
   "metadata": {},
   "source": [
    "### Input word2Vec for SVM and Perceptron \n",
    "\n",
    "1. First convert each word in the tokenized_reviews column to word vector using Google Word2Vec pretrained model for both test and train datasets. Each word here is now represented by an array of size 300 \n",
    "\n",
    "2. We then take average of the word vectors. The averaged vector is the representation of entire review. Each review now is represented by an array of size 300. This is performed for both test and train datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3269fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect = np.array(\n",
    "    [np.array(\n",
    "        [wv[i] for i in ls if i in words_google]\n",
    "    ) for ls in train_data[\"tokenized_reviews\"]])\n",
    "X_test_vect = np.array(\n",
    "    [np.array(\n",
    "        [wv[i] for i in ls if i in words_google]\n",
    "    ) for ls in test_data[\"tokenized_reviews\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e013b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect_avg = []\n",
    "for v in X_train_vect:\n",
    "    if v.size:\n",
    "        X_train_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train_vect_avg.append(np.zeros(300, dtype=float))\n",
    "        \n",
    "X_test_vect_avg = []\n",
    "for v in X_test_vect:\n",
    "    if v.size:\n",
    "        X_test_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test_vect_avg.append(np.zeros(300, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1acbff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data[\"star_rating\"].astype('int').to_numpy()\n",
    "y_test = test_data[\"star_rating\"].astype('int').to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4d4504",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e6938",
   "metadata": {},
   "source": [
    "##### LinearSVC function from sklearn python package is used. Input to SVM is the averaged word2Vec that was generated in previous step Parameters set for the model are - \n",
    "\n",
    "1. random_state=12\n",
    "2. tol=1e-3\n",
    "3. class_weight = 'balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4980a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.5040181691125087,\n",
       "  'recall': 0.72125,\n",
       "  'f1-score': 0.5933772110242698,\n",
       "  'support': 4000},\n",
       " '2': {'precision': 0.3791291291291291,\n",
       "  'recall': 0.2525,\n",
       "  'f1-score': 0.30312124849939975,\n",
       "  'support': 4000},\n",
       " '3': {'precision': 0.39994532531437943,\n",
       "  'recall': 0.36575,\n",
       "  'f1-score': 0.3820840950639854,\n",
       "  'support': 4000},\n",
       " '4': {'precision': 0.4425026214610276,\n",
       "  'recall': 0.3165,\n",
       "  'f1-score': 0.3690424136423262,\n",
       "  'support': 4000},\n",
       " '5': {'precision': 0.5949342234439426,\n",
       "  'recall': 0.7575,\n",
       "  'f1-score': 0.6664467172550314,\n",
       "  'support': 4000},\n",
       " 'accuracy': 0.4827,\n",
       " 'macro avg': {'precision': 0.46410589369219757,\n",
       "  'recall': 0.4827,\n",
       "  'f1-score': 0.4628143370970025,\n",
       "  'support': 20000},\n",
       " 'weighted avg': {'precision': 0.4641058936921975,\n",
       "  'recall': 0.4827,\n",
       "  'f1-score': 0.4628143370970025,\n",
       "  'support': 20000}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_linear_clf = LinearSVC(\n",
    "    random_state=12, \n",
    "    tol=1e-3, \n",
    "    class_weight = 'balanced')\n",
    "svm_linear_clf.fit(X_train_vect_avg, y_train)\n",
    "\n",
    "y_test_pred = svm_linear_clf.predict(X_test_vect_avg)\n",
    "report = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "svm_wordvec_accuracy = report[\"accuracy\"]\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9b68a",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d0bea3",
   "metadata": {},
   "source": [
    "##### Perceptron function from sklearn python package is used. Input to Perceptron is the averaged word2Vec that was generated in previous step Parameters set for the model are - \n",
    "\n",
    "1. random_state=1\n",
    "2. tol=1e-5\n",
    "3. max_iter = 30000\n",
    "4. validation_fraction = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f0055284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.5212617288880016,\n",
       "  'recall': 0.65275,\n",
       "  'f1-score': 0.5796425796425797,\n",
       "  'support': 4000},\n",
       " '2': {'precision': 0.3736434108527132,\n",
       "  'recall': 0.1205,\n",
       "  'f1-score': 0.1822306238185255,\n",
       "  'support': 4000},\n",
       " '3': {'precision': 0.32136268601648016,\n",
       "  'recall': 0.65325,\n",
       "  'f1-score': 0.430797131316462,\n",
       "  'support': 4000},\n",
       " '4': {'precision': 0.47985347985347987,\n",
       "  'recall': 0.03275,\n",
       "  'f1-score': 0.06131523519775334,\n",
       "  'support': 4000},\n",
       " '5': {'precision': 0.5586180857088918,\n",
       "  'recall': 0.73975,\n",
       "  'f1-score': 0.6365494245455523,\n",
       "  'support': 4000},\n",
       " 'accuracy': 0.4398,\n",
       " 'macro avg': {'precision': 0.45094787826391336,\n",
       "  'recall': 0.43979999999999997,\n",
       "  'f1-score': 0.37810699890417454,\n",
       "  'support': 20000},\n",
       " 'weighted avg': {'precision': 0.4509478782639133,\n",
       "  'recall': 0.4398,\n",
       "  'f1-score': 0.37810699890417454,\n",
       "  'support': 20000}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_clf = Perceptron(\n",
    "    random_state=1, \n",
    "    tol=1e-5, \n",
    "    max_iter = 30000, \n",
    "    validation_fraction = 0.1\n",
    ")\n",
    "perceptron_clf.fit(X_train_vect_avg, y_train)\n",
    "\n",
    "y_test_pred = perceptron_clf.predict(X_test_vect_avg)\n",
    "report = classification_report(\n",
    "    y_test, \n",
    "    y_test_pred, \n",
    "    output_dict = True)\n",
    "perceptron_wordvec_accuracy = report[\"accuracy\"]\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a25f49",
   "metadata": {},
   "source": [
    "### What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)\n",
    "Both the models were run on same training and testing dataset. Also, same preprocessing techniques were applied on both dataset. This was to ensure that both model performances can be compared directly with each other. \n",
    "\n",
    "###### Details regarding input features that were compared against in the Experiment - \n",
    "TF-IDF - Term frequency Inverse Document Frequency is numerical statistic that provides us with measure of how important a word is to a document given a corpus. It builds on a simple idea which is bag of words. \n",
    "\n",
    "Word2Vec - Word2Vec is implementation of advanced techniques such Continuous bag of words and Skipgram. It takes in dataset as input. It first creates vocabulary and then generates vectors to represent them. In this process, it also encodes word similarities, semantic similarities and word co-occurences. \n",
    "\n",
    "#### SVM Model\n",
    "Following is the summary of the SVM model performance with TF-IDF and Word2Vec as input features\n",
    "\n",
    "| Measure    | TF-IDF          |  Word2Vec\n",
    "| ---------- | --------------- | ---------------\n",
    "| Accuracy   | 0.50            |  0.4827\n",
    "| Precision  | 0.4870          |  0.4641\n",
    "| Recall     | 0.501           |  0.4827\n",
    "| F1 Score   | 0.4895          |  0.4628\n",
    "\n",
    "It can be seen that irrespective of whether the input features were generated through simple techniques or advanced techniques, Support Vector Machine model has performed more or less the same. Infact, the accuracy, precision, recall and F1-Score obtained with simple technique TF-IDF is slightly greater than the one that was tried with Word2vec.\n",
    "\n",
    "#### Perceptron Model\n",
    "Following is the summary of the Perceptron model performance with TF-IDF and Word2Vec as input features\n",
    "\n",
    "| Measure    | TF-IDF          |  Word2Vec\n",
    "| ---------- | --------------- | ---------------\n",
    "| Accuracy   | 0.4232          |  0.4398\n",
    "| Precision  | 0.4232          |  0.4509\n",
    "| Recall     | 0.4257          |  0.4397\n",
    "| F1 Score   | 0.4238          |  0.3781\n",
    "\n",
    "Similar results as SVM are seen in Perceptron as well. Irrespective of technique used for input feature word representation, Perceptron model has performed similar in both cases. We can notice that for tf-idf input features, the values of accuracy, precision, recall and f1 scores are consistent. For word2Vec input, the f1 score is on lower end.\n",
    "\n",
    "The overall learning from this -\n",
    "1. TF-IDF features were extracted based on Amazon Review Dataset and was catered to pick work context, co-occurences and similarities from it. And probably this could be one of the reasons why it performed better when compared to Google word2Vec input features which is generic and not catered for Amazon dataset specifically.\n",
    "2. Advanced or complex techniques don't really mean better performance or outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec056b6",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384ad0f",
   "metadata": {},
   "source": [
    "### a) To generate the input features, use the average Word2Vec vectors similar to the “Simple models” section and train the neural network. Report accuracy values on the testing split for your MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8913b0",
   "metadata": {},
   "source": [
    "##### The following steps are performed on both train dataset and test dataset\n",
    "\n",
    "1. Tokenized reviews are converted to word2Vectors first. Each word is a vector of size 300 \n",
    "2. The vectors for each review are averaged. After this step, each review is represented by a single vector of size 300\n",
    "3. If there are any samples with review vectors as nan/null, these are identified and dropped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b31bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_X_train = train_data.copy()\n",
    "fnn_X_test = test_data.copy()\n",
    "\n",
    "fnn_X_train[\"fnn_vectors\"] = fnn_X_train['tokenized_reviews'].apply(\n",
    "    lambda wordlist: [wv[i] for i in wordlist if i in words_google])\n",
    "fnn_X_train['averaged_vectors'] = fnn_X_train.loc[:, 'fnn_vectors']\n",
    "indexlist = []\n",
    "\n",
    "for index, row in fnn_X_train.iterrows():\n",
    "    count = 0\n",
    "    vectors = np.zeros(300)\n",
    "    vectors_list = row['fnn_vectors']\n",
    "    if vectors_list is not None and len(vectors_list) >= 1:\n",
    "        for v in vectors_list:\n",
    "            count = count + 1\n",
    "            vectors = np.add(vectors, v)\n",
    "        average_vector = np.divide(vectors, len(vectors_list))\n",
    "        fnn_X_train.at[index, \"averaged_vectors\"] = average_vector\n",
    "    else:\n",
    "        indexlist.append(index)\n",
    "\n",
    "fnn_X_train = fnn_X_train[~fnn_X_train.index.isin(indexlist)]\n",
    "\n",
    "fnn_X_test[\"fnn_vectors\"] = fnn_X_test['tokenized_reviews'].apply(\n",
    "    lambda wordlist: [wv[i] for i in wordlist if i in words_google])\n",
    "fnn_X_test['averaged_vectors'] = fnn_X_test.loc[:, 'fnn_vectors']\n",
    "indexlist = []\n",
    "\n",
    "for index, row in fnn_X_test.iterrows():\n",
    "    count = 0\n",
    "    vectors = np.zeros(300)\n",
    "    vectors_list = row['fnn_vectors']\n",
    "    if vectors_list is not None and len(vectors_list) >= 1:\n",
    "        for v in vectors_list:\n",
    "            count = count + 1\n",
    "            vectors = np.add(vectors, v)\n",
    "        average_vector = np.divide(vectors, len(vectors_list))\n",
    "        fnn_X_test.at[index, \"averaged_vectors\"] = average_vector\n",
    "    else:\n",
    "        indexlist.append(index)\n",
    "\n",
    "fnn_X_test = fnn_X_test[~fnn_X_test.index.isin(indexlist)]\n",
    "\n",
    "fnn_X_train.reset_index(drop=True, inplace = True)\n",
    "fnn_X_test.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c619d1a",
   "metadata": {},
   "source": [
    "##### FNNTensorDataset and Dataloader\n",
    "\n",
    "1. It has helper function __getitem__ that fetches item sample from dataframe based on index value. \n",
    "2. Both train and test dataset are casted to FNNTensorDataset class\n",
    "3. This is done so as to be able to define DataLoader\n",
    "4. A dataloader takes in FNNTensorDataset class type and generates batches (batch size is set to 100) of data that are selected randomly (SubsetRandomSampler) and returns it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b443adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNTensorDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data.loc[index, 'averaged_vectors']\n",
    "        label = self.data.loc[index, 'star_rating']\n",
    "        return torch.from_numpy(features).float(), label - 1\n",
    "    \n",
    "    def __getindexlist__(self):\n",
    "        return list(self.data.index.values)\n",
    "    \n",
    "fnn_X_train_tensor = FNNTensorDataset(fnn_X_train)\n",
    "fnn_X_test_tensor = FNNTensorDataset(fnn_X_test)\n",
    "\n",
    "num_of_workers = 0\n",
    "batch_size = 100\n",
    "valid_size = 0.2\n",
    "\n",
    "indices = list(range(len(fnn_X_train_tensor)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    fnn_X_train_tensor, \n",
    "    batch_size=batch_size, \n",
    "    sampler=SubsetRandomSampler(indices)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d45a0",
   "metadata": {},
   "source": [
    "##### FNNNet Module \n",
    "\n",
    "1. We have defined a Feedforward Neural Network model with 2 hidden layers of size 50 and 10 \n",
    "2. As the input is averaged vector, the size of input layer is 300\n",
    "3. As the output is classification of reviews into 5 ratings, the output layer size is 5\n",
    "\n",
    "##### Cross Entropy loss is used. <br>\n",
    "\n",
    "##### For Optimizer, Stochastic Gradient Descent and Adam were tried with different learning rates (0.01, 0.001, 0.005, 0.0001, 0.0005). The best accuracy was achieved with Adam optimizer with a learning rate of 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29fe9ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNNNet(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FNNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FNNNet, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = nn.Linear(1*300, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1*300)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "fnn = FNNNet()\n",
    "print(fnn)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(fnn.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e51a0f",
   "metadata": {},
   "source": [
    "##### Training on the dataset is done for 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d0602fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 20\n",
      "Completed: 40\n",
      "Completed: 60\n",
      "Completed: 80\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch.manual_seed(42)\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    fnn.train()\n",
    "        \n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = fnn(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    fnn.eval()\n",
    "    \n",
    "    if(epoch != 0 and epoch%20 == 0):\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        print(\"Completed: \" + str(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922be25a",
   "metadata": {},
   "source": [
    "##### Predict function \n",
    "\n",
    "1. The predict function takes in the model and dataloader. \n",
    "2. The function iterates through every sample in dataloader and makes prediction using the model \n",
    "3. For each prediction, the output is vector of size 5. The index of the highest value in the vector is selected\n",
    "4. A list of predictions and actual data values is created and returned back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64418e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fnn(fnn_model, dataloader):\n",
    "    prediction_list = []\n",
    "    actual_list = []\n",
    "    for data, target in dataloader:\n",
    "        outputs = fnn_model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1) \n",
    "        prediction_list.append(predicted.cpu())\n",
    "        actual_list.append(target)\n",
    "    return prediction_list, actual_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eb1d3d",
   "metadata": {},
   "source": [
    "##### Test Accuracy \n",
    "1. Test Loader is created with test dataset \n",
    "2. A function call to predict_fnn is made with model and test loader \n",
    "3. The predictions and actual labels obtained are compared with each other using sklearn classification_report\n",
    "4. Accuracy for the model is reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09e46518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of FNN model with Average Vectors : 49.76732549412059%\n",
      "\n",
      "Classification Report\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.5589732711073399,\n",
       "  'recall': 0.65875,\n",
       "  'f1-score': 0.6047739270140005,\n",
       "  'support': 4000},\n",
       " '1': {'precision': 0.38153930430019173,\n",
       "  'recall': 0.3485985985985986,\n",
       "  'f1-score': 0.3643258794298418,\n",
       "  'support': 3996},\n",
       " '2': {'precision': 0.40625776011919545,\n",
       "  'recall': 0.4092046023011506,\n",
       "  'f1-score': 0.4077258566978193,\n",
       "  'support': 3998},\n",
       " '3': {'precision': 0.4492753623188406,\n",
       "  'recall': 0.41116116116116114,\n",
       "  'f1-score': 0.42937410165947987,\n",
       "  'support': 3996},\n",
       " '4': {'precision': 0.6704776422764228,\n",
       "  'recall': 0.660575719649562,\n",
       "  'f1-score': 0.6654898499558694,\n",
       "  'support': 3995},\n",
       " 'accuracy': 0.4976732549412059,\n",
       " 'macro avg': {'precision': 0.4933046680243981,\n",
       "  'recall': 0.49765801634209444,\n",
       "  'f1-score': 0.4943379229514021,\n",
       "  'support': 19985},\n",
       " 'weighted avg': {'precision': 0.49330023508080384,\n",
       "  'recall': 0.4976732549412059,\n",
       "  'f1-score': 0.494342795003278,\n",
       "  'support': 19985}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    fnn_X_test_tensor, \n",
    "    batch_size=fnn_X_test_tensor.__len__())\n",
    "predictions, actuals = predict_fnn(fnn, test_loader)\n",
    "predictions = predictions[0].numpy()\n",
    "actuals = actuals[0].numpy()\n",
    "report = classification_report(\n",
    "    actuals, \n",
    "    predictions, \n",
    "    output_dict=True)\n",
    "print(\"Accuracy of FNN model with Average Vectors : \" + str(\n",
    "    report[\"accuracy\"]*100) + \"%\\n\")\n",
    "print(\"Classification Report\")\n",
    "fnna_wordvec_average_accuracy = report[\"accuracy\"]\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad3cdff",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75057429",
   "metadata": {},
   "source": [
    "### b) To generate the input features, concatenate the first 10 Word2Vec vectors for each review as the input feature (x = [WT1, ..., WT10]) and train the neural network. Report the accuracy value on the testing split for your MLP model. What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c429c",
   "metadata": {},
   "source": [
    "##### The following steps are performed on both train dataset and test dataset\n",
    "\n",
    "1. Tokenized reviews are converted to word2Vectors first. Each word is a vector of size 300 \n",
    "2. Each review is truncated to have only 10 words \n",
    "3. The vectors are then concatenated. If the size of vectors is less than 300*10, then zeros are appended at the end\n",
    "4. After this step, each review is represented by a single vector of size 300*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "549f84d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_X_train = train_data.copy()\n",
    "fnn_X_test = test_data.copy()\n",
    "\n",
    "fnn_X_train[\"fnn_vectors\"] = fnn_X_train['tokenized_reviews'].apply(\n",
    "    lambda wordlist: [wv[i] for i in wordlist if i in words_google])\n",
    "fnn_X_train[\"reduced_reviews\"] = fnn_X_train['fnn_vectors'].apply(\n",
    "    lambda x: x[:10])\n",
    "\n",
    "for index, row in fnn_X_train.iterrows():\n",
    "    data_list = row['reduced_reviews']\n",
    "    if data_list is None:\n",
    "        fnn_X_train.at[index, \"reduced_reviews\"] = np.asarray(\n",
    "            [np.zeros(300)] * 10)\n",
    "    elif len(data_list) < 10:\n",
    "        data_list.extend([np.zeros(300)] * (10-len(data_list)))\n",
    "        fnn_X_train.at[index, \"reduced_reviews\"] = np.asarray(\n",
    "            data_list)\n",
    "    else:\n",
    "        fnn_X_train.at[index, \"reduced_reviews\"] = np.asarray(\n",
    "            data_list)\n",
    "fnn_X_train[\"star_rating\"] = fnn_X_train[\"star_rating\"].astype(\n",
    "    'int').to_numpy()\n",
    "\n",
    "\n",
    "fnn_X_test[\"fnn_vectors\"] = fnn_X_test['tokenized_reviews'].apply(\n",
    "    lambda wordlist: [wv[i] for i in wordlist if i in words_google])\n",
    "fnn_X_test[\"reduced_reviews\"] = fnn_X_test['fnn_vectors'].apply(\n",
    "    lambda x: x[:10])\n",
    "\n",
    "for index, row in fnn_X_test.iterrows():\n",
    "    data_list = row['reduced_reviews']\n",
    "    if data_list is None:\n",
    "        fnn_X_test.at[index, \"reduced_reviews\"] = np.asarray(\n",
    "            [np.zeros(300)] * 10)\n",
    "    elif len(data_list) < 10:\n",
    "        data_list.extend([np.zeros(300)] * (10-len(data_list)))\n",
    "        fnn_X_test.at[index, \"reduced_reviews\"] = np.asarray(\n",
    "            data_list)\n",
    "    else:\n",
    "        fnn_X_test.at[index, \"reduced_reviews\"] = np.asarray(\n",
    "            data_list)\n",
    "fnn_X_test[\"star_rating\"] = fnn_X_test[\"star_rating\"].astype(\n",
    "    'int').to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ddaa6",
   "metadata": {},
   "source": [
    "##### FNNBTensorDataset and Dataloader\n",
    "\n",
    "1. It has helper function __getitem__ that fetches item sample from dataframe based on index value. \n",
    "2. Both train and test dataset are casted to FNNBTensorDataset class\n",
    "3. This is done so as to be able to define DataLoader\n",
    "4. A dataloader takes in FNNBTensorDataset class type and generates batches (batch size is set to 100) of data that are selected randomly (SubsetRandomSampler) and returns it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3272d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNBTensorDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data.loc[index, 'reduced_reviews']\n",
    "        label = self.data.loc[index, 'star_rating']\n",
    "        return torch.from_numpy(features).float(), label - 1\n",
    "    \n",
    "    def __getindexlist__(self):\n",
    "        return list(self.data.index.values)\n",
    "\n",
    "fnn_X_train_tensor = FNNBTensorDataset(fnn_X_train)\n",
    "fnn_X_test_tensor = FNNBTensorDataset(fnn_X_test)\n",
    "\n",
    "num_of_workers = 0\n",
    "batch_size = 100\n",
    "valid_size = 0.2\n",
    "\n",
    "indices = list(range(len(fnn_X_train_tensor)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    fnn_X_train_tensor, \n",
    "    batch_size=batch_size, \n",
    "    sampler=SubsetRandomSampler(indices)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f74110",
   "metadata": {},
   "source": [
    "##### FNNBNet Module \n",
    "\n",
    "1. We have defined a Feedforward Neural Network model with 2 hidden layers of size 50 and 10 \n",
    "2. As the input is concatenated vectors, the size of input layer is 3000\n",
    "3. As the output is classification of reviews into 5 ratings, the output layer size is 5\n",
    "\n",
    "##### Cross Entropy loss is used. <br>\n",
    "\n",
    "##### For Optimizer, Stochastic Gradient Descent and Adam were tried with different learning rates (0.01, 0.001, 0.005, 0.0001, 0.0005). The best accuracy was achieved with Adam optimizer with a learning rate of 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecb3dede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNNBNet(\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FNNBNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FNNBNet, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = nn.Linear(10*300, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 10*300)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "fnnb = FNNBNet()\n",
    "print(fnnb)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(fnnb.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86f2d8c",
   "metadata": {},
   "source": [
    "##### Training on the dataset is done for 4 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da26f00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 2\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 4\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch.manual_seed(42)\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    fnnb.train()\n",
    "        \n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = fnnb(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    fnnb.eval()\n",
    "   \n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    if(epoch != 0 and epoch%2 == 0):\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        print(\"Completed: \" + str(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e055a6",
   "metadata": {},
   "source": [
    "##### Test Accuracy \n",
    "1. Test Loader is created with test dataset \n",
    "2. A function call to predict_fnn is made with model and test loader \n",
    "3. The predictions and actual labels obtained are compared with each other using sklearn classification_report\n",
    "4. Accuracy for the model is reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbcb73ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of FNN model with Concatenated Vectors : 43.26%\n",
      "\n",
      "Classification Report\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.4684126349460216,\n",
       "  'recall': 0.58575,\n",
       "  'f1-score': 0.5205509886691846,\n",
       "  'support': 4000},\n",
       " '1': {'precision': 0.33868894601542415,\n",
       "  'recall': 0.2635,\n",
       "  'f1-score': 0.296400449943757,\n",
       "  'support': 4000},\n",
       " '2': {'precision': 0.3593204561321853,\n",
       "  'recall': 0.386,\n",
       "  'f1-score': 0.3721827166445703,\n",
       "  'support': 4000},\n",
       " '3': {'precision': 0.4092773745661092,\n",
       "  'recall': 0.32425,\n",
       "  'f1-score': 0.3618356814060538,\n",
       "  'support': 4000},\n",
       " '4': {'precision': 0.5461538461538461,\n",
       "  'recall': 0.6035,\n",
       "  'f1-score': 0.573396674584323,\n",
       "  'support': 4000},\n",
       " 'accuracy': 0.4326,\n",
       " 'macro avg': {'precision': 0.42437065156271725,\n",
       "  'recall': 0.43260000000000004,\n",
       "  'f1-score': 0.42487330224957776,\n",
       "  'support': 20000},\n",
       " 'weighted avg': {'precision': 0.4243706515627173,\n",
       "  'recall': 0.4326,\n",
       "  'f1-score': 0.42487330224957776,\n",
       "  'support': 20000}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    fnn_X_test_tensor, \n",
    "    batch_size=fnn_X_test_tensor.__len__())\n",
    "predictions, actuals = predict_fnn(fnnb, test_loader)\n",
    "predictions = predictions[0].numpy()\n",
    "actuals = actuals[0].numpy()\n",
    "report = classification_report(\n",
    "    actuals, \n",
    "    predictions, \n",
    "    output_dict=True)\n",
    "print(\"Accuracy of FNN model with Concatenated Vectors : \" + str(\n",
    "    report[\"accuracy\"]*100) + \"%\\n\")\n",
    "fnnb_wordvec_cat_accuracy = report[\"accuracy\"]\n",
    "print(\"Classification Report\")\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4af8db",
   "metadata": {},
   "source": [
    "### What do you conclude by comparing accuracy values you obtained with FeedForward Neural Networks model with those obtained in the “’Simple Models” section.\n",
    "\n",
    "Both the models were run on same training and testing dataset. Also, same preprocessing techniques were applied on both dataset. This was to ensure that all model performances can be compared directly with each other. <br>\n",
    "\n",
    "Following is the summary of the SVM, Perceptron and FNN model performances with TF-IDF and Word2Vec as input features\n",
    "\n",
    "| Model                              | Accuracy         \n",
    "| ---------------------------------- | --------------- \n",
    "| SVM                                | 0.4827 \n",
    "| Perceptron                         | 0.4398\n",
    "| FNN with average word vectors      | 0.4976\n",
    "| FNN with concatenated word vectors | 0.4326\n",
    "\n",
    "SVM, Perceptron and FNN models were tried with average word2Vec input features. Among these, SVM and FNN have performed well and have almost same accuracy with slight difference. FNN has been able to perform 2% better when compared to SVM. But given the computation resources that FNN might require for larger dataset, its a trade off between accuracy and computational resources that one has to make. \n",
    "\n",
    "FNN was also tried by considering 10 words from each review concatenated together. The first 10 words were considered by checking if they existed in the Google Word2Vec dictionary. If the word didn't exist, it was discarded. All reviews that were less than 10 word length were padded with zeros vectors. The performance of this model was very low compared to SVM and FNN that were tried with average word2Vec. \n",
    "\n",
    "Another observation is that models with average word vectors as input have performed better compared to models which had concatenated word vectors as input. The intuition behind average word vectors for review representation is that, if most words in the review are similar or co-occur, then their vectors are closer to each other in space. Taking average of these vectors, results in single vector that would be mostly at the center of all these vectors. This single vector will be good representation of the entire review. \n",
    "\n",
    "Overall, Perceptron and FNN (with concatenated word vectors) performed poorly. SVM and FNN (with average word vectors) have performed better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d8f5b4",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a98021f",
   "metadata": {},
   "source": [
    "### (a) Train a simple RNN for sentiment analysis. You can consider an RNN cell with the hidden state size of 20. To feed your data into our RNN, limit the maximum review length to 20 by truncating longer reviews and padding shorter reviews with a null value (0). Report accuracy values on the testing split for your RNN model. What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41d05f",
   "metadata": {},
   "source": [
    "##### The following steps are performed on both train dataset and test dataset\n",
    "\n",
    "1. Tokenized reviews are converted to word2Vectors first. Each word is a vector of size 300 \n",
    "2. The vectors for each review reduced to 20 words. After this step, each review is represented by a vector of 20 vectors each of size 300 (300*20)\n",
    "3. If there are any samples with review vectors size lesser than 300*20, these are padded with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df81a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_X_train = train_data.copy()\n",
    "rnn_X_test = test_data.copy()\n",
    "\n",
    "rnn_X_train[\"rnn_vectors\"] = rnn_X_train['tokenized_reviews'].apply(\n",
    "    lambda wordlist: [wv[i] for i in wordlist if i in words_google])\n",
    "rnn_X_train[\"reduced_rnn_vectors\"] = rnn_X_train['rnn_vectors'].apply(\n",
    "    lambda x: x[:20])\n",
    "\n",
    "rnn_X_train[\"star_rating\"] = rnn_X_train[\"star_rating\"].astype(\n",
    "    'int').to_numpy()\n",
    "\n",
    "for index, row in rnn_X_train.iterrows():\n",
    "    data_list = row['reduced_rnn_vectors']\n",
    "    if data_list is None:\n",
    "        rnn_X_train.at[index, \"reduced_rnn_vectors\"] = [np.zeros(\n",
    "            300)] * 20\n",
    "    elif len(data_list) < 20:\n",
    "        data_list.extend([np.zeros(300)] * (20-len(data_list)))\n",
    "        rnn_X_train.at[index, \"reduced_rnn_vectors\"] = data_list\n",
    "        \n",
    "        \n",
    "rnn_X_test[\"rnn_vectors\"] = rnn_X_test['tokenized_reviews'].apply(\n",
    "    lambda wordlist: [wv[i] for i in wordlist if i in words_google])\n",
    "rnn_X_test[\"reduced_rnn_vectors\"] = rnn_X_test['rnn_vectors'].apply(\n",
    "    lambda x: x[:20])\n",
    "\n",
    "rnn_X_test[\"star_rating\"] = rnn_X_test[\"star_rating\"].astype(\n",
    "    'int').to_numpy()\n",
    "\n",
    "for index, row in rnn_X_test.iterrows():\n",
    "    data_list = row['reduced_rnn_vectors']\n",
    "    if data_list is None:\n",
    "        rnn_X_test.at[index, \"reduced_rnn_vectors\"] = [np.zeros(\n",
    "            300)] * 20\n",
    "    elif len(data_list) < 20:\n",
    "        data_list.extend([np.zeros(300)] * (20-len(data_list)))\n",
    "        rnn_X_test.at[index, \"reduced_rnn_vectors\"] = data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d0eec5",
   "metadata": {},
   "source": [
    "##### RNNTensorDataset and Dataloader\n",
    "\n",
    "1. It has helper function __getitem__ that fetches item sample from dataframe based on index value. \n",
    "2. Both train and test dataset are casted to RNNTensorDataset class\n",
    "3. This is done so as to be able to define DataLoader\n",
    "4. A dataloader takes in RNNTensorDataset class type and generates batches (batch size is set to 100) of data that are selected randomly (SubsetRandomSampler) and returns it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b4fcd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTensorDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data.loc[index, 'reduced_rnn_vectors']\n",
    "        label = self.data.loc[index, 'star_rating']\n",
    "        return features, label, label-1\n",
    "    \n",
    "    def __getindexlist__(self):\n",
    "        return list(self.data.index.values)\n",
    "    \n",
    "rnn_X_train_tensor = RNNTensorDataset(rnn_X_train)\n",
    "rnn_X_test_tensor = RNNTensorDataset(rnn_X_test)\n",
    "\n",
    "num_of_workers = 0\n",
    "batch_size = 100\n",
    "valid_size = 0.2\n",
    "\n",
    "indices = list(range(len(rnn_X_train_tensor)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    rnn_X_train_tensor, \n",
    "    batch_size=batch_size, \n",
    "    sampler=SubsetRandomSampler(indices)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b6d9db",
   "metadata": {},
   "source": [
    "##### RNN Module \n",
    "\n",
    "1. We have defined a Recurrent Neural Network model with hidden size of 20\n",
    "2. Size of input layer is input size + hidden size\n",
    "3. As the output is classification of reviews into 5 ratings, the output layer size is 5\n",
    "\n",
    "##### Negative Log Likelihood loss is used. <br>\n",
    "\n",
    "##### For Optimizer, Stochastic Gradient Descent and Adam were tried with different learning rates (0.01, 0.001, 0.005, 0.0001, 0.0005). The best accuracy was achieved with Adam optimizer with a learning rate of 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fdd6b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "\n",
    "hidden_size = 20\n",
    "output_size = 5\n",
    "input_size = 300\n",
    "rnn = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "all_categories = [1, 2, 3, 4, 5]\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = torch.max(output,dim=1)\n",
    "    return top_i\n",
    "\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386be544",
   "metadata": {},
   "source": [
    "##### Training on the dataset is done for 120 epochs\n",
    "\n",
    "Each input is fed in sequence to the model. With every input, output and hidden state is generated. This hidden state is then again fed with next input word in the sequence. The output of final word in the review is used to find the classification done by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea0f5b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch 10\n",
      "Completed epoch 20\n",
      "Completed epoch 30\n",
      "Completed epoch 40\n",
      "Completed epoch 50\n",
      "Completed epoch 60\n",
      "Completed epoch 70\n",
      "Completed epoch 80\n",
      "Completed epoch 90\n",
      "Completed epoch 100\n",
      "Completed epoch 110\n",
      "Completed epoch 120\n"
     ]
    }
   ],
   "source": [
    "def train(class_data, class_data_index, review):\n",
    "    hidden = rnn.initHidden(batch_size)\n",
    "    optimizer.zero_grad()\n",
    "    for i in range(len(review)):\n",
    "        review_tensor = torch.from_numpy(\n",
    "            np.asarray(review[i])).float()\n",
    "        output, hidden = rnn(review_tensor, hidden)\n",
    "\n",
    "    loss = criterion(output, class_data_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "n_iters = 120\n",
    "print_every = 10\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    torch.manual_seed(42)\n",
    "    for data, target, target_index in train_loader:\n",
    "        output, loss = train(target, target_index, data)\n",
    "    if (iter%print_every == 0):\n",
    "        print(\"Completed epoch \" + str(iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388716f",
   "metadata": {},
   "source": [
    "##### Test Accuracy \n",
    "1. Test Loader is created with test dataset \n",
    "2. A function call to test_accuracy is made with loader and batch siexe\n",
    "3. The predictions and actual labels obtained are compared with each other using custom written function categoryFromOutput and torch.eq\n",
    "4. Accuracy for the model is reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c20bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.45405\n"
     ]
    }
   ],
   "source": [
    "def evaluateRNN(review, size):\n",
    "    hidden = rnn.initHidden(size)\n",
    "\n",
    "    for i in range(len(review)):\n",
    "        review_tensor = torch.from_numpy(\n",
    "            np.asarray(review[i])).float()\n",
    "        output, hidden = rnn(review_tensor, hidden)\n",
    "    return output\n",
    "\n",
    "def test_accuracy(loader, size):\n",
    "    compare_list = []\n",
    "    for data, target, target_index in loader:\n",
    "        output = evaluateRNN(data, size)\n",
    "        prediction_index = categoryFromOutput(output)\n",
    "        compare = torch.eq(prediction_index, target_index)\n",
    "        compare_list = compare.tolist()\n",
    "    return compare_list\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    rnn_X_test_tensor, \n",
    "    batch_size=rnn_X_test_tensor.__len__())\n",
    "compare_list = test_accuracy(\n",
    "    test_loader, rnn_X_test_tensor.__len__())\n",
    "rnn_accuracy = sum(compare_list)/len(compare_list)\n",
    "print(\"accuracy: \" + str(rnn_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16617d47",
   "metadata": {},
   "source": [
    "### What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models.\n",
    "\n",
    "All the models were run on same training and testing dataset. Also, same preprocessing techniques were applied on both dataset. This was to ensure that all model performances can be compared directly with each other. <br>\n",
    "\n",
    "Following is the summary of the RNN and FNN model performance with TF-IDF and Word2Vec as input features\n",
    "\n",
    "| Model                              | Accuracy         \n",
    "| ---------------------------------- | --------------- \n",
    "| RNN with 20 words                  | 0.4540\n",
    "| FNN with average word vectors      | 0.4976\n",
    "| FNN with concatenated word vectors | 0.4326\n",
    "\n",
    "RNN has performed better compared to FNN with concatenated words. This could be because \n",
    "1. FNN made use of 10 words while RNN is considering 20 words. This leads to larger features and more data to work with for the RNN model and hence find better classification boundaries\n",
    "2. RNN works in a sequence and is capable of maintaining historical data in form of hidden states for predictions and is good at handling shorter sequences\n",
    "\n",
    "Though RNN has performed better compared to FNN with concatenated words, it has not performed so well when compared to FNN with average word vectors. The reason for this could include \n",
    "1. FNN worked with all words in the review and took average of it whereas RNN worked with just 20 words from each review. A lot of important information can be lost leading to lower accuracy for RNN\n",
    "2. RNN has issue of vanishing or exploding gradients that might cause significant issues in the learning of the model during back propagation\n",
    "\n",
    "The conclusion is that FNN with average word vectors performs best till now among all the models that have been compared.\n",
    "\n",
    "By varying hyperparameters of the models such as number of hidden layers, hidden layer size, activation functions used etc., it is probably possible to gain better accuracy. Accuracy values also depend on the data cleaning steps that have been included. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d081e25c",
   "metadata": {},
   "source": [
    "### (b) Repeat part (a) by considering a gated recurrent unit cell. What do you conclude by comparing accuracy values you obtain with those obtained using simple RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1811977",
   "metadata": {},
   "source": [
    "##### The following steps are performed on both train dataset and test dataset\n",
    "\n",
    "1. Tokenized reviews are converted to word2Vectors first. Each word is a vector of size 300 \n",
    "2. The vectors for each review reduced to 20 words. After this step, each review is represented by a vector of 20 vectors each of size 300 (300*20)\n",
    "3. If there are any samples with review vectors size lesser than 300*20, these are padded with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77d0847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_X_train = train_data.copy()\n",
    "rnn_X_test = test_data.copy()\n",
    "\n",
    "rnn_X_train[\"gru_vectors\"] = rnn_X_train['tokenized_reviews'].apply(\n",
    "    lambda wordlist: [wv[i] for i in wordlist if i in words_google])\n",
    "rnn_X_train[\"reduced_gru_vectors\"] = rnn_X_train['gru_vectors'].apply(\n",
    "    lambda x: x[:20])\n",
    "rnn_X_train[\"star_rating\"] = rnn_X_train[\"star_rating\"].astype(\n",
    "    'int').to_numpy()\n",
    "\n",
    "for index, row in rnn_X_train.iterrows():\n",
    "    data_list = row['reduced_gru_vectors']\n",
    "    if data_list is None:\n",
    "        rnn_X_train.at[index, \"reduced_gru_vectors\"] = [np.zeros(\n",
    "            300)] * 20\n",
    "    elif len(data_list) < 20:\n",
    "        data_list.extend([np.zeros(300)] * (20-len(data_list)))\n",
    "        rnn_X_train.at[index, \"reduced_gru_vectors\"] = data_list\n",
    "        \n",
    "        \n",
    "rnn_X_test[\"gru_vectors\"] = rnn_X_test['tokenized_reviews'].apply(\n",
    "    lambda wordlist: [wv[i] for i in wordlist if i in words_google])\n",
    "rnn_X_test[\"reduced_gru_vectors\"] = rnn_X_test['gru_vectors'].apply(\n",
    "    lambda x: x[:20])\n",
    "rnn_X_test[\"star_rating\"] = rnn_X_test[\"star_rating\"].astype(\n",
    "    'int').to_numpy()\n",
    "\n",
    "for index, row in rnn_X_test.iterrows():\n",
    "    data_list = row['reduced_gru_vectors']\n",
    "    if data_list is None:\n",
    "        rnn_X_test.at[index, \"reduced_gru_vectors\"] = [np.zeros(\n",
    "            300)] * 20\n",
    "    elif len(data_list) < 20:\n",
    "        data_list.extend([np.zeros(300)] * (20-len(data_list)))\n",
    "        rnn_X_test.at[index, \"reduced_gru_vectors\"] = data_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872d811",
   "metadata": {},
   "source": [
    "##### GRUTensorDataset and Dataloader\n",
    "\n",
    "1. It has helper function __getitem__ that fetches item sample from dataframe based on index value. \n",
    "2. Both train and test dataset are casted to GRUTensorDataset class\n",
    "3. This is done so as to be able to define DataLoader\n",
    "4. A dataloader takes in GRUTensorDataset class type and generates batches (batch size is set to 100) of data that are selected randomly (SubsetRandomSampler) and returns it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c25fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUTensorDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data.loc[index, 'reduced_gru_vectors']\n",
    "        label = self.data.loc[index, 'star_rating']\n",
    "        return torch.from_numpy(\n",
    "            np.asarray(features)).float(), label, label - 1\n",
    "    \n",
    "    def __getindexlist__(self):\n",
    "        return list(self.data.index.values)\n",
    "    \n",
    "rnn_X_train_tensor = GRUTensorDataset(rnn_X_train)\n",
    "rnn_X_test_tensor = GRUTensorDataset(rnn_X_test)\n",
    "\n",
    "num_of_workers = 0\n",
    "batch_size = 100\n",
    "valid_size = 0.2\n",
    "\n",
    "indices = list(range(len(rnn_X_train_tensor)))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    rnn_X_train_tensor, \n",
    "    batch_size=batch_size, \n",
    "    sampler=SubsetRandomSampler(indices)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d961a",
   "metadata": {},
   "source": [
    "##### GRUNet Module \n",
    "\n",
    "1. We have defined a Gated Recurrent Neural Network model \n",
    "2. As the output is classification of reviews into 5 ratings, the output layer size is 5\n",
    "3. Softmax is used for last layer\n",
    "\n",
    "##### Negative Log Likelihood loss is used. <br>\n",
    "\n",
    "##### For Optimizer, Stochastic Gradient Descent and Adam were tried with different learning rates (0.01, 0.001, 0.005, 0.0001, 0.0005). The best accuracy was achieved with Adam optimizer with a learning rate of 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed019c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUNet(\n",
      "  (gru): GRU(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, \n",
    "                 output_dim, n_layers):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, \n",
    "                          n_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.softmax(self.fc(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, \n",
    "                            self.hidden_dim).zero_()\n",
    "        return hidden\n",
    "\n",
    "hidden_size = 20\n",
    "output_size = 5\n",
    "input_size = 300\n",
    "n_layers = 1\n",
    "gru = GRUNet(input_size, hidden_size, output_size, n_layers)\n",
    "print(gru)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b293b",
   "metadata": {},
   "source": [
    "##### Training on the dataset is done for 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f4f3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch.manual_seed(42)\n",
    "    train_loss = 0.0\n",
    "    gru.train()\n",
    "    for data, target, target_index in train_loader:\n",
    "        h = gru.init_hidden(batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output, h = gru(data, h.data)\n",
    "        loss = criterion(output, target_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    gru.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e59b4",
   "metadata": {},
   "source": [
    "##### Test Accuracy \n",
    "1. Test Loader is created with test dataset \n",
    "2. A function call to test_accuracy is made with loader and batch siexe\n",
    "3. The predictions and actual labels obtained are compared with each other using custom written function categoryFromOutput and torch.eq\n",
    "4. Accuracy for the model is reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ced68a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.51105\n"
     ]
    }
   ],
   "source": [
    "def evaluateGRU(review, size):\n",
    "    hidden = gru.init_hidden(size)\n",
    "    output, hidden = gru(review, hidden)\n",
    "    return output\n",
    "\n",
    "def test_accuracy(loader, size):\n",
    "    compare_list = []\n",
    "    for data, target, target_index in loader:\n",
    "        output = evaluateGRU(data, size)\n",
    "        prediction_index = categoryFromOutput(output)\n",
    "        compare = torch.eq(prediction_index, target_index)\n",
    "        compare_list = compare.tolist()\n",
    "    return compare_list\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    rnn_X_test_tensor, \n",
    "    batch_size=rnn_X_test_tensor.__len__())\n",
    "compare_list = test_accuracy(\n",
    "    test_loader, \n",
    "    rnn_X_test_tensor.__len__())\n",
    "gru_accuracy = sum(compare_list)/len(compare_list)\n",
    "print(\"accuracy: \" + str(gru_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999c8619",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e49ccb",
   "metadata": {},
   "source": [
    "### What do you conclude by comparing accuracy values you obtain with those obtained using simple RNN.\n",
    "\n",
    "All the models were run on same training and testing dataset. Also, same preprocessing techniques were applied on both dataset. This was to ensure that all model performances can be compared directly with each other. <br>\n",
    "\n",
    "Following is the summary of the model performances \n",
    "\n",
    "| Model                              | Accuracy         \n",
    "| ---------------------------------- | --------------- \n",
    "| RNN with 20 words                  | 0.4540\n",
    "| Gated RNN with 20 words.           | 0.51105\n",
    "\n",
    "\n",
    "We can conclude that Gated Recurrent Unit has performed better than RNN and other models (FNN, SVM, Perceptron) that were tried. \n",
    "\n",
    "Some of the possible reasons for this - \n",
    "1. RNN has issue of vanishing or exploding gradients. This can hinder the learning during back propagation in training phase\n",
    "2. Also, RNN has issue of short term memory problem for long sequences. \n",
    "\n",
    "Gated Recurrent Unit overcomes these issues and has capability of handling long term memory dependencies with control mechanisms for information flow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c8930c",
   "metadata": {},
   "source": [
    "##### Note: \n",
    "\n",
    "All of the Neural Networks models that were tried were with limited hidden layers and nodes in hidden layers with some restrictions on the input size for each review. There are possibilites of obtaining better accuracy with deeper networks and thorough hyper paramater tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59967b2",
   "metadata": {},
   "source": [
    "# Report All Model Accuracies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1421932",
   "metadata": {},
   "source": [
    "### Simple Models with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "02a3352a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM Model with TF-IDF inputs : 50%\n",
      "Accuracy of Perceptron Model with TF-IDF inputs : 43%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of SVM Model with TF-IDF inputs : \" + \"50%\")\n",
    "print(\"Accuracy of Perceptron Model with TF-IDF inputs : \" + \"43%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99f9aff",
   "metadata": {},
   "source": [
    "### Simple Models with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2e392e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM Model with word2Vec inputs : 48.27%\n",
      "Accuracy of Perceptron Model with word2Vec inputs : 43.980000000000004%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of SVM Model with word2Vec inputs : \" + str(\n",
    "    svm_wordvec_accuracy*100) + \"%\")\n",
    "print(\"Accuracy of Perceptron Model with word2Vec inputs : \" + str(\n",
    "    perceptron_wordvec_accuracy*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158563a",
   "metadata": {},
   "source": [
    "### FeedForward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "67a06e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of FNN Model with averaged word2Vec inputs : 49.76732549412059%\n",
      "Accuracy of FNN Model with concatenated word2Vec inputs : 43.26%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of FNN Model with averaged word2Vec inputs : \" + str(\n",
    "    fnna_wordvec_average_accuracy*100) + \"%\")\n",
    "print(\"Accuracy of FNN Model with concatenated word2Vec inputs : \" + str(\n",
    "    fnnb_wordvec_cat_accuracy*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488e9b6f",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a5e857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RNN Model with word2Vec inputs : 45.405%\n",
      "Accuracy of GRU Model with word2Vec inputs : 51.105000000000004%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of RNN Model with word2Vec inputs : \" + str(\n",
    "    rnn_accuracy*100) + \"%\")\n",
    "print(\"Accuracy of GRU Model with word2Vec inputs : \" + str(\n",
    "    gru_accuracy*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e976a",
   "metadata": {},
   "source": [
    "# REFERENCES \n",
    "\n",
    "1. https://blog.floydhub.com/gru-with-pytorch/\n",
    "2. https://github.com/georgeyiasemis/Recurrent-Neural-Networks-from-scratch-using-PyTorch\n",
    "3. https://towardsdatascience.com/implementation-of-rnn-lstm-and-gru-a4250bf6c090\n",
    "4. https://github.com/georgeyiasemis/Recurrent-Neural-Networks-from-scratch-using-PyTorch/blob/main/rnnmodels.py\n",
    "5. https://github.com/kaustubhhiware/LSTM-GRU-from-scratch\n",
    "6. https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "7. https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook\n",
    "8. https://www.youtube.com/watch?v=WEV61GmmPrk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
